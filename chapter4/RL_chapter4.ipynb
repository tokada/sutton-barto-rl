{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 『強化学習』第4章 動的計画法\n",
    "\n",
    "（参考資料（DeepMindレクチャー）: [video](https://www.youtube.com/watch?v=Nd1-UUMVfz4) [slides](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/DP.pdf)）\n",
    "\n",
    "- Dynamic Programming（動的計画法）とは？\n",
    " - Dynamic: 問題における直列的あるいは時系列的な要素\n",
    " - Programming: 最適な方策を得ること\n",
    "\n",
    "\n",
    "\n",
    "- DPは以下の2つの要素をもつ問題に適用できる\n",
    " - 部分構造最適性（optimal substructure）がある\n",
    " - 部分問題重複性（overlapping subproblems）がある\n",
    "   - 解決法が再利用できる（value functionを適用できる）\n",
    "\n",
    "\n",
    "\n",
    "- 完全なモデルを前提とする点、計算量が膨大である点などから、強化学習では実用的ではないが、理論的には重要。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 方策評価（Policy Evaluation）\n",
    "\n",
    "- 方策評価（Policy Evaluation）: 任意の方策 ${\\pi}$ に対する状態価値関数 $V^{\\pi}$ を計算する\n",
    "- 反復方策評価（Iterative Policy Evaluation）: 完全バックアップ（現在評価している方策で実行可能なすべての1ステップ遷移に対して、現在のsの価値と1ステップ先の即時報酬の期待値を使って新たなsの価値を計算）により元の価値 $V_{k}$ に対して新しい近似価値関数 $V_{k+1}$ を得て、これを繰り返す\n",
    "- スイープ操作（sweep）: 元の価値 $V_{k}$ を保存しながら新しい価値 $V_{k+1}$ を計算するのではなく、バックアップを行って得た新しい価値 $V_{k+1}(s)$ で元の価値 $V_{k}(s)$ を直接上書きしていく。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 例4.1「格子世界」（図4.2）\n",
    "\n",
    "実装コード: [RL_chapter4_ex4.1_tokada.ipynb](RL_chapter4_ex4.1_tokada.ipynb) を参照\n",
    "\n",
    "- 4×4の格子世界の例で、図4.2の反復方策評価を出力する。\n",
    "- 評価結果に対してGreedy戦略をとることで、最適方策を得られている。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 方策改善（Policy Improvement）\n",
    "\n",
    "- 方策改善: 元の方策の価値関数に従ってgreedyな行動を選択することで、その方策を改善するような新しい方策を作り出す過程（p.101）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 方策反復（Policy Iteration）\n",
    "\n",
    "例4.1「格子世界」では方策評価した結果に対してgreedy戦略を1回とることで最適方策を得られた。\n",
    "\n",
    "実際には、1回で最適方策を得られるケースは少なく、以下のように方策評価と方策改善を繰り返して方策を改善させていく必要がある。\n",
    "\n",
    "- 方策 $\\pi_0$ からスタートする\n",
    "- 方策 $\\pi_k$ を入力として、\n",
    "  - 方策評価（E）し、 $V^{\\pi_k}$ を得る\n",
    "  - 方策改善（I）し、 $\\pi_{k+1}$ を得る\n",
    "  - 得た $\\pi_{k+1}$ を入力として、方策が最適化するまで上記を繰り返す"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 例4.2「Jackのレンタカー会社」（図4.4）\n",
    "\n",
    "実装コード: [RL_chapter4_ex4.2_adliska.ipynb](RL_chapter4_ex4.2_adliska.ipynb) を参照\n",
    "\n",
    "- 図4.4\n",
    "  - 方策改善するごとに、方策 $\\pi$ が変化し $\\pi_4$ で収束することが分かる。\n",
    "  - また、第1営業所と第2営業所の車両台数に応じて、得られる収益（ドル建て）が状態価値関数として示されている。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 価値反復（Value Iteration）\n",
    "\n",
    "- 価値反復: 方策評価を繰り返し行わないように、1回のスイープ操作で方策評価を打ち切る。\n",
    "- スイープ操作でのバックアップ時に、すべての行動に対する最大値を用いる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 例4.3「ギャンブラーの問題」（図4.6）\n",
    "\n",
    "実装コード: [RL_chapter4_ex4.3_bikestra.ipynb](RL_chapter4_ex4.3_bikestra.ipynb) を参照\n",
    "\n",
    "練習問題4.8: [zyxue/gambler.ipynb](zyxue/gambler.ipynb) を参照"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 非同期動的計画法\n",
    "\n",
    "- これまで述べてきたDP手法は同期的、すなわちMDPの状態集合全体に対してバックアップを行う必要があった。\n",
    "- 非同期DP手法は、状態集合全体ではなく、状態を任意の順序で選んで価値のバックアップを行う。\n",
    "- In-Place DP、Prioritized DP, Real-time DPであることが基本的なアイディアである。（[DeepMindレクチャー](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/DP.pdf#page=28)より）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 一般化方策反復（Generalized Policy Iteration; GPI）\n",
    "\n",
    "- 方策評価と方策改善の2つの過程の相互作用は、必ずしも交互に行う必要はなく、様々な手法がある。\n",
    "- この相互作用の考え方を一般的概念として一般化方策反復（Generalized Policy Iteration; GPI）と呼ぶ。\n",
    "- 評価と改善はそれぞれ競争関係にある（互いに反対方向へ引き合っている）\n",
    "  - ＃このあたり、GANの考え方と似ている。。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考資料\n",
    "\n",
    "- 例題の実装\n",
    "  - https://github.com/zyxue/sutton-barto-rl-exercises\n",
    "  - https://github.com/dennybritz/reinforcement-learning\n",
    "  - https://github.com/vinayh/rl-sutton-barto\n",
    "  - https://github.com/OsamaElHariri/Reinforcment-Learning-Sutton-and-Barto-Solution\n",
    "  - https://github.com/adliska/reinforcement_learning\n",
    "  - https://github.com/bikestra/suttonbarto\n",
    "- David Silver @ DeepMind によるUCLでのレクチャー\n",
    "  - 資料 http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/DP.pdf\n",
    "  - 動画 https://www.youtube.com/watch?v=Nd1-UUMVfz4\n",
    "- ブログ\n",
    "  - [強化学習： ノート9 - クッキーの日記](http://cookie-box.hatenablog.com/entry/2016/04/04/235825) 93～98ページ\n",
    "  - [強化学習： ノート10 - クッキーの日記](http://cookie-box.hatenablog.com/entry/2016/04/10/223239) 98～118ページ\n",
    "- その他資料\n",
    "  - http://jnns.org/pastpage/niss/2000/lecturenote/lecturenote_koike.pdf"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
